---
title: "DP、DDP、ZeRO"
date: 2024-11-2
mathjax: true
categories: 
  - distributed training
---

本篇文章重点是ZeRO，但开始之前先来介绍一下DP与DDP。

- DP（Data Parallelism）：最早的数据并行模式，一般采用参数服务器(Parameters Server)这一编程框架。实际中多用于单机多卡。

- DDP（Distributed Data Parallelism）：分布式数据并行，采用Ring AllReduce的通讯方式，实际中多用于多机场景。

## DP

### 过程

描述如下图，
![](https://pic1.zhimg.com/v2-10696b17568f69cfd31912c208ee191e_1440w.jpg)
简单说就是每个节点上都保存着完整模型，数据拆分成mini-batch输入，计算出梯度后发送给ps，然后ps将更新后的参数发回。

### 存在问题
简易的框架，性能的缺陷：
- **额外显存开销**：每块GPU上都有一份完整的模型。
- **通讯开销**：每次梯度更新都需要跟Server进行通讯，并且通讯量大，Server的带宽会成为整个系统的计算瓶颈(当然这可以通过参数分片来解决)。

> 除此之外，梯度的更新也是硬伤。同步导致计算机资源利用率低下，异步训练出的模型与串行训练的模型不一致，并且收敛速度受到影响。

>我认为之所以DP更多用在一机多卡场景，是因为卡之间有着专门的通讯库（nccl之类的），通讯效率高掩盖了通讯量大这个问题，但放在多机多卡只能通过网络传输的场景下，传输瓶颈一下子就暴露出来了。

## DDP

就目前来看，DDP只是使用Ring AllReduce来解决了多机通信问题，让单卡通讯两下降到$2 (N - 1)  \frac{\Phi}{N}$。

## ZeRO

ZeRO解决了DP中遗留的显存开销问题，他的核心思想就是 **通讯换显存**。

### 显存消耗

#### 数据类别

首先我们先了解什么占用了显存，即在训练中GPU都存着什么。
存储内容主要分为两大块：**Model States**和**Residual States**，Model States指和模型本身相关的，是必须存储的内容，具体包括：
- **optimizer states**：Adam优化算法中的momentum和variance。
- **gradients**
- **parameters**

Residual states指非模型必须的，在训练过程中产生的额外内容，具体包括：
- **activation**
- **temporary buffers**：临时存储。例如把梯度发送到某块GPU上做聚合是产生的存储。
- **unusable fragment memory**：碎片化的内存空间。

#### 存储总大小

进一步，如果模型参数$W$大小是$\Phi$，那么每一类数据占了多大的空间呢？这涉及到我们训练具体过程，下图是混合精度训练的步骤。

![](https://picx.zhimg.com/v2-72f68c3cd3e0dd87c2afb1e14b3f6587_1440w.jpg)

- 存储一份fp32的parameter，momentum和variance（统称model states）。
- 在forward开始之前，额外开辟一块存储空间，将fp32 parameter减半到fp16parameter。
- 正常做forward和backward，在此之间产生的activation和gradients，都用fp16进行存储。
- 用fp16 gradients去更新fp32下的model states。
- 当模型收敛后，fp32的parameter就是最终的参数输出。

现在我们可以得出模型训练需要的存储大小了，

![](https://pic4.zhimg.com/v2-2fa670488fcc2408bd27bdcfec283d33_1440w.jpg)

这里以Adam优化为例子，所以才会出现momentum、variance。
>这里并没有将activation纳入范围，原因是：
>- activation不仅与模型参数相关，还与batch size相关。
>- 其次activation的存储不是必须的。存储activation只是为了在用链式法则做backward的过程中，计算梯度更快一些。但你永远可以通过只保留最初的输入X，重新做forward来得到每一层的activation。

### ZeRO-DP

ZeRO-DP是对model states存储的优化。

在这里我们首先回顾一下训练流程：forward、backward，得出梯度，收集其他GPU上的梯度做平均，平均梯度与优化器结合，得出最终梯度，再根据最新梯度更新参数。

#### $P_{os} + P_{g}$：optimizer states、gradients分割
首先，先从optimizer state开始，将它按照GPU均等分：

![](https://pic2.zhimg.com/v2-6a314b96490b51cffc5aa4b693ea32c1_1440w.jpg)

对os分割后的训练流程应该是这样的：forward、backward，得出完整梯度，收集其他gpu上的梯度做平均，但此时只保留自己维护的那一部分，...
到这里发现，我们根本就不用维护一份完全的梯度，不属于该GPU的梯度只要存放在临时区域，做完通信后丢弃即可。


![](https://pic2.zhimg.com/v2-29290e0e77d330b6e0e4d7d3fb417a73_1440w.jpg)

此时，数据并行的流程如下：
1. 每块GPU上存一份完整的参数。将一个batch分成3分，每块GPU各吃一份，做完forward、backward后得到一份完整的梯度。（下图绿色+白色）
2. 对梯度做一次Reduce-scatter，获得平均梯度。每块GPU最终只需要绿色部分，其他可以丢弃。单卡通讯量为$\Phi$。

>了解Ring-AllReduce就知道$\Phi$怎么算的了，但这其实是一个估算。

![](https://picx.zhimg.com/v2-3dd79addc9cbe6eb3d22a49037f6e087_1440w.jpg)

3. 每块GPU更新完参数W后，需要对W做一次All-gather。单卡通讯量为$\Phi$。

最终的单卡通讯量为$2\Phi$。

#### $P_{os} + P_{g} + P_{w}$

再进一步优化，把parameter也抛弃。

![](https://pic1.zhimg.com/v2-ade8d5f51d46b23ef7b25cf73248853c_1440w.jpg)

简单讲就是在每次forward、backward时，把W做一次All-gather取回，然后**做完后立马把不是自己维护的那一部分W丢弃**，每次都额外产生$\Phi$的通讯量，但是在最后不需要把完整的参数W聚合回来，因此最终单卡通讯量时$3\Phi$。

最终战绩：
![](https://pic2.zhimg.com/v2-94b9574e2fff5d5bee5d09f5d35926bf_1440w.jpg)

### ZeRO-Offload

ZeRO-Offload，将训练阶段的某些模型状态的存储和计算下放到内存和CPU。
有两条前提：
- 不能让CPU过多参与计算，以免计算成为瓶颈
- 不能让GPU和CPU之间的通讯成为瓶颈

为了找到最优的offload策略，我们参考以下数据流图：

![](https://pic4.zhimg.com/v2-3d3a9ce68a740dfd5f0210ab2f8aa03d_1440w.jpg)

我们现在要做的事就是沿着边把数据流图切分为两个部分，对应着CPU、GPU。

ZeRO-Offload切分的思路是：

图中有四个计算节点：FWD、BWD、Param update、float2half，前两个的复杂度大约是$O(MB)$，$B$是batch size，后两个则是$O(M)$。为了充分利用GPU，我们选择将前两个放在GPU，后两个不但计算量小还需要与Adam交互，所以放在CPU上。

![](https://pic4.zhimg.com/v2-b9f59b045c1629983fb3e94e692bc457_1440w.jpg)

现在的计算流程是，在GPU上面进行FWD、BWD，将梯度传给CPU，进行参数更新，再将更新后的参数传给GPU。为了提高效率，可以将通信和计算并行，GPU在BWD阶段，边算边传，增大通信效率可以按bucket大小传，CPU参数更新也是同样的。

![](https://pic3.zhimg.com/v2-57ab3768637af499bda329bbecb1ce1a_1440w.jpg)

>但是有一个问题：当batch size不够大的时候，CPU的计算就会成为瓶颈，**一种方法是让CPU在某个节点更新参数的时候延迟一步，然后在下一轮FWD的时候CPU就开始计算更新的参数**，这样子CPU相当于跟GPU并行计算了，具体如下：
>
>前N-1步，不进行延迟，避免早期训练不稳定，模型无法收敛，在第N步，CPU拿到GPU计算的梯度后，不更新参数，相当于GPU空算了一步，在N+1步GPU接着用N-1的参数计算，此时CPU开始根据刚才第N步的梯度计算。
> ![](https://pic3.zhimg.com/v2-d9aa4eba2b96960e13364a7f004462e0_1440w.jpg)
> 经实验，这种方法几乎不会影响模型的训练。