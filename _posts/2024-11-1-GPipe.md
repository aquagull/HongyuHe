---
title: "GPipe"
date: 2024-11-1
mathjax: true
categories: 
  - distributed training
---

在讲流水线并行这个技术之前，我们先来看这个旨在解决的问题。
大模型迅速发展，参数早已爆炸，一张24GB的卡根本塞不进一个完整的模型，因此人们想到将模型按层拆分，改将层放在GPU上。

![](https://pic2.zhimg.com/v2-233703a2792e0299a81439a0cf7bbd53_1440w.jpg)

此时，模型做一轮forward和backward的过程如下：

![](https://pic2.zhimg.com/v2-365ac7c9e9ea7d5b93dcb90656c131a7_1440w.jpg)

从图中不难看出主要问题：**GPU利用率低**：在每一个时刻只有一个GPU在计算，其他的GPU都在空转。具体低，到底有多低呢，我们不妨量化一下。

![](https://picx.zhimg.com/v2-8db7ffbb378266ad68cac5401664a471_1440w.jpg)

我们假设有$K$块GPU，而在单块GPU上做一次forward和backward的时间为：$t_{fb} = (t_{f} + t_{b})$。则：
- 图中灰色长方形面积：$K \cdot K \cdot t_{fb}$
- 图中实际在做forward和backward的面积为：$K \cdot t_{fb}$ 
- 图中阴影部分面积为：$K \cdot K \cdot t_{fb} - K \cdot t_{fb} = (K - 1) \cdot K \cdot t_{fb}$
- 则图像阴影部分的占比为：$(K - 1) \cdot K \cdot t_{fb} / K \cdot K \cdot t_{fb}$

据此，我们定义bubble的时间复杂度为： $O(\frac{K-1}{K})$，**当$K$越大，即GPU数目越多的时候，bubble的时间复杂度接近1，即GPU资源都被浪费完了。**

## 流水线并行

### 切分micro-batch

主要的核心思想：把数据的粒度变细，计算了一批数据后立马进行下一批。因此，我们又将mini-batch再划分成micro-batch。假设拆成$M$份，

![](https://pic1.zhimg.com/v2-accf297316558a8a0328a0defadcf00c_1440w.jpg)

此时：
- 总面积为： $(K - 1 + M) \cdot t_{fb}$
- 计算的面积： $K \cdot M \cdot t_{fb}$
- bubble的面积： $(K - 1 + M) \cdot t_{fb} - K \cdot M \cdot t_{fb} = (K - 1) \cdot t_{fb}$

则bubble的时间复杂度为：$O(\frac{K-1}{K+M-1})$。$M$越大，即拆分越多的mirco-batch我们的计算率就越大，但实际上我们还需要考虑其他因素。更多的mirco-batch可能会让GPU的通信开销增大，并且每个micro-batch都需要保留其对应的中间激活值和梯度，拆分过细，会导致显存占用增加。

在梯度更新上，Gpipe是同步的，PipeDream是异步的。异步方法更进一步降低了GPU的空转时间比。虽然PipeDream设计更精妙些，但是Gpipe因为其“够用”和浅显易懂，更受大众欢迎（torch的pp接口就基于Gpipe）。